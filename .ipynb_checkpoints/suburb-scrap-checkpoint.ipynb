{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T23:48:39.890136Z",
     "start_time": "2017-09-11T23:48:38.526667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1771"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/houses.csv')\n",
    "df['a'] = zip(df.suburb, df.postcode)\n",
    "\n",
    "def a(x):\n",
    "    s, p = x\n",
    "    return s + '-VIC-' + str(p)\n",
    "    \n",
    "l = df.a.map(a).unique()\n",
    "l = [x.replace(' ', '-') for x in l]\n",
    "l.remove('MELBOURNE-(3004)-VIC-3004')\n",
    "\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T19:10:17.938447Z",
     "start_time": "2017-09-11T14:01:30.731546Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def getinfo(url):\n",
    "    browser = webdriver.Chrome(executable_path=\"chromedriver/chromedriver\")\n",
    "    browser.get(url)\n",
    "    \n",
    "    try:\n",
    "        # 1 bed unit\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/a\").click()\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/ul/li[1]/a\").click()\n",
    "        s = browser.find_elements_by_class_name('hood-buying-stat')\n",
    "        _1bed_unit_median_price = s[0].text\n",
    "        _1bed_unit_days = s[1].text\n",
    "        _1bed_unit_clearance = s[2].text\n",
    "        _1bed_unit_sold = s[3].text\n",
    "\n",
    "        # 2 bed unit\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/a\").click()\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/ul/li[2]/a\").click()\n",
    "        s = browser.find_elements_by_class_name('hood-buying-stat')\n",
    "        _2bed_unit_median_price = s[0].text\n",
    "        _2bed_unit_days = s[1].text\n",
    "        _2bed_unit_clearance = s[2].text\n",
    "        _2bed_unit_sold = s[3].text\n",
    "\n",
    "        # 3 bed unit\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/a\").click()\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/ul/li[3]/a\").click()\n",
    "        s = browser.find_elements_by_class_name('hood-buying-stat')\n",
    "        _3bed_unit_median_price = s[0].text\n",
    "        _3bed_unit_days = s[1].text\n",
    "        _3bed_unit_clearance = s[2].text\n",
    "        _3bed_unit_sold = s[3].text\n",
    "\n",
    "        # 2 bed house\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/a\").click()\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/ul/li[4]/a\").click()\n",
    "        s = browser.find_elements_by_class_name('hood-buying-stat')\n",
    "        _2bed_house_median_price = s[0].text\n",
    "        _2bed_house_days = s[1].text\n",
    "        _2bed_house_clearance = s[2].text\n",
    "        _2bed_house_sold = s[3].text\n",
    "\n",
    "        # 3 bed house\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/a\").click()\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/ul/li[5]/a\").click()\n",
    "        s = browser.find_elements_by_class_name('hood-buying-stat')\n",
    "        _3bed_house_median_price = s[0].text\n",
    "        _3bed_house_days = s[1].text\n",
    "        _3bed_house_clearance = s[2].text\n",
    "        _3bed_house_sold = s[3].text\n",
    "\n",
    "        # 4 bed house\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/a\").click()\n",
    "        browser.find_element_by_xpath(\"//*[@id='market-data-buying']/div/header/div/ul/li[6]/a\").click()\n",
    "        s = browser.find_elements_by_class_name('hood-buying-stat')\n",
    "        _4bed_house_median_price = s[0].text\n",
    "        _4bed_house_days = s[1].text\n",
    "        _4bed_house_clearance = s[2].text\n",
    "        _4bed_house_sold = s[3].text\n",
    "    except:\n",
    "        _1bed_unit_median_price = np.nan\n",
    "        _1bed_unit_days = np.nan\n",
    "        _1bed_unit_clearance = np.nan\n",
    "        _1bed_unit_sold = np.nan\n",
    "        \n",
    "        _2bed_unit_median_price = np.nan\n",
    "        _2bed_unit_days = np.nan\n",
    "        _2bed_unit_clearance = np.nan\n",
    "        _2bed_unit_sold = np.nan\n",
    "        \n",
    "        _3bed_unit_median_price = np.nan\n",
    "        _3bed_unit_days = np.nan\n",
    "        _3bed_unit_clearance = np.nan\n",
    "        _3bed_unit_sold = np.nan\n",
    "        \n",
    "        _2bed_house_median_price = np.nan\n",
    "        _2bed_house_days = np.nan\n",
    "        _2bed_house_clearance = np.nan\n",
    "        _2bed_house_sold = np.nan\n",
    "        \n",
    "        _3bed_house_median_price = np.nan\n",
    "        _3bed_house_days = np.nan\n",
    "        _3bed_house_clearance = np.nan\n",
    "        _3bed_house_sold = np.nan\n",
    "        \n",
    "        _4bed_house_median_price = np.nan\n",
    "        _4bed_house_days = np.nan\n",
    "        _4bed_house_clearance = np.nan\n",
    "        _4bed_house_sold = np.nan\n",
    "\n",
    "    html = browser.page_source\n",
    "    soup = Soup(html, 'lxml')\n",
    "\n",
    "    # close browser\n",
    "    browser.quit()\n",
    "\n",
    "    name = soup.find('header', class_='hood-location-header').text\n",
    "    \n",
    "    try:\n",
    "        travel2csb = soup.find('p', id='travelTrain').text\n",
    "    except:\n",
    "        travel2csb = np.nan\n",
    "\n",
    "    chart = soup.find_all('div', class_='demographics-chart-data')\n",
    "    \n",
    "    try:\n",
    "        age = chart[0].text\n",
    "    except:\n",
    "        age = np.nan\n",
    "        \n",
    "    try:\n",
    "        dependency = chart[1].text\n",
    "        dependency_family = dependency.split()[0]\n",
    "        dependency_single = dependency.split()[1]\n",
    "    except:\n",
    "        dependency_family = np.nan\n",
    "        dependency_single = np.nan\n",
    "        \n",
    "    try:\n",
    "        occupation1 = soup.find_all('span', class_='bar-label')[0].text\n",
    "        occupation2 = soup.find_all('span', class_='bar-label')[1].text\n",
    "        occupation3 = soup.find_all('span', class_='bar-label')[2].text\n",
    "    except:\n",
    "        occupation1 = np.nan\n",
    "        occupation2 = np.nan\n",
    "        occupation3 = np.nan\n",
    "        \n",
    "    try:\n",
    "        fullyowned = chart[3].text.split()[0]\n",
    "        purchasing = chart[3].text.split()[1]\n",
    "        renting = chart[3].text.split()[2]\n",
    "    except:\n",
    "        fullyowned = np.nan\n",
    "        purchasing = np.nan\n",
    "        renting = np.nan\n",
    "\n",
    "    school = soup.find_all('div', class_='school-catchment__school-title-and-tags-container')\n",
    "    numberofschool = len(school)\n",
    "\n",
    "    result = [name, travel2csb, age, dependency_family, dependency_single,\n",
    "              occupation1, occupation2, occupation3, fullyowned, purchasing,\n",
    "              renting, numberofschool, \n",
    "              _1bed_unit_median_price, _1bed_unit_days, _1bed_unit_clearance, _1bed_unit_sold,\n",
    "              _2bed_unit_median_price, _2bed_unit_days, _2bed_unit_clearance, _2bed_unit_sold,\n",
    "              _3bed_unit_median_price, _3bed_unit_days, _3bed_unit_clearance, _3bed_unit_sold,\n",
    "              _2bed_house_median_price, _2bed_house_days, _2bed_house_clearance, _2bed_house_sold,\n",
    "              _3bed_house_median_price, _3bed_house_days, _3bed_house_clearance, _3bed_house_sold,\n",
    "              _4bed_house_median_price, _4bed_house_days, _4bed_house_clearance, _4bed_house_sold]\n",
    "    return result\n",
    "\n",
    "base_url = 'https://www.domain.com.au/suburb-profile/'\n",
    "\n",
    "columns = ['suburbname', 'travel2csb', 'age', 'dependency_family', 'dependency_single',\n",
    "              'occupation1', 'occupation2', 'occupation3', 'fullyowned', 'purchasing',\n",
    "              'renting', 'numberofschool', \n",
    "              '_1bed_unit_median_price', '_1bed_unit_days', '_1bed_unit_clearance', '_1bed_unit_sold',\n",
    "              '_2bed_unit_median_price', '_2bed_unit_days', '_2bed_unit_clearance', '_2bed_unit_sold',\n",
    "              '_3bed_unit_median_price', '_3bed_unit_days', '_3bed_unit_clearance', '_3bed_unit_sold',\n",
    "              '_2bed_house_median_price', '_2bed_house_days', '_2bed_house_clearance', '_2bed_house_sold',\n",
    "              '_3bed_house_median_price', '_3bed_house_days', '_3bed_house_clearance', '_3bed_house_sold',\n",
    "              '_4bed_house_median_price', '_4bed_house_days', '_4bed_house_clearance', '_4bed_house_sold']\n",
    "\n",
    "with open('suburb2.csv','wb') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow(columns)\n",
    "        \n",
    "        for i in l[127:]:\n",
    "            url = base_url + i\n",
    "            print url\n",
    "            writer.writerow(getinfo(url))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:21.231411Z",
     "start_time": "2017-09-12T00:22:21.210233Z"
    }
   },
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('data/suburb.csv')\n",
    "d2 = pd.read_csv('data/suburb2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:21.481818Z",
     "start_time": "2017-09-12T00:22:21.474809Z"
    }
   },
   "outputs": [],
   "source": [
    "d = pd.concat([d1, d2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:21.704528Z",
     "start_time": "2017-09-12T00:22:21.698084Z"
    }
   },
   "outputs": [],
   "source": [
    "d=d.reset_index()\n",
    "d.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:21.958560Z",
     "start_time": "2017-09-12T00:22:21.953042Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitnamepost(x):\n",
    "    s = x.split('\\n')\n",
    "    return s[1]\n",
    "\n",
    "d['name'] = d.suburbname.map(splitnamepost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:22.191513Z",
     "start_time": "2017-09-12T00:22:22.184762Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitnamepost(x):\n",
    "    s = x.split('\\n')\n",
    "    p = s[2].split(', ')[1]\n",
    "    return p\n",
    "\n",
    "d['post'] = d.suburbname.map(splitnamepost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:22.464406Z",
     "start_time": "2017-09-12T00:22:22.458706Z"
    }
   },
   "outputs": [],
   "source": [
    "d.drop('suburbname', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:22.800627Z",
     "start_time": "2017-09-12T00:22:22.796346Z"
    }
   },
   "outputs": [],
   "source": [
    "d.drop('travel2csb', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:23.249000Z",
     "start_time": "2017-09-12T00:22:23.242433Z"
    }
   },
   "outputs": [],
   "source": [
    "def getage(x):\n",
    "    try:\n",
    "        s = x.split('\\n')\n",
    "        return s[1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "d.age = d.age.map(getage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:23.630861Z",
     "start_time": "2017-09-12T00:22:23.584427Z"
    }
   },
   "outputs": [],
   "source": [
    "def extrainfo(x):\n",
    "    try:\n",
    "        re = x.split('\\n')[1]\n",
    "        if re == '-':\n",
    "            return np.nan\n",
    "        else:\n",
    "            return re.replace('$','')\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "d.iloc[:, 10:-2] = d.iloc[:, 10:-2].applymap(extrainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:22:23.956173Z",
     "start_time": "2017-09-12T00:22:23.913152Z"
    }
   },
   "outputs": [],
   "source": [
    "d.iloc[:, 10:-2] = d.iloc[:, 10:-2].fillna(0)\n",
    "\n",
    "def ton(x):\n",
    "    if x == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        if 'k' in x:\n",
    "            return float(x.replace('k', '')) * 1000\n",
    "        if 'm' in x:\n",
    "            return float(x.replace('m', '')) * 1000000\n",
    "\n",
    "d.iloc[:, 10:-2] = d.iloc[:, 10:-2].applymap(ton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T00:23:10.828586Z",
     "start_time": "2017-09-12T00:23:10.806277Z"
    }
   },
   "outputs": [],
   "source": [
    "d.to_csv('sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
