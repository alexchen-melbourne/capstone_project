{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "import csv, requests, time\n",
    "import couchdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_one_page(url):\n",
    "    result = {}\n",
    "    with requests.Session() as session:\n",
    "        response = session.post(url, data={'start':0})\n",
    "        soup = Soup(response.content, 'lxml')\n",
    "        \n",
    "        # left wrap\n",
    "        left = soup.find('div', class_='left-wrap')\n",
    "        \n",
    "        result['date'] = left.find('span', class_='status-label label-sold').text.strip().encode('ascii','ignore')\n",
    "        result['address'] = left.find('h1').text.strip().encode('ascii','ignore')\n",
    "        result['price'] = left.find('span', class_='h4 pricedisplay truncate-single').text.strip().encode('ascii','ignore')\n",
    "        \n",
    "        # right wrap\n",
    "        right = soup.find('div', class_='listing-features alt').text\n",
    "        result['bed'] = right.split()[0].encode('ascii','ignore')\n",
    "        result['bath'] = right.split()[2].encode('ascii','ignore')\n",
    "        result['park'] = right.split()[4].encode('ascii','ignore')\n",
    "        \n",
    "        # land\n",
    "        try:\n",
    "            result['landsize'] = soup.find('ul', class_='list-horizontal details cfix ').text.strip().encode('ascii','ignore')\n",
    "        except:\n",
    "            result['landsize'] = None\n",
    "        \n",
    "        # title\n",
    "        result['title'] = soup.find('h4', class_='h5 lowlight').text.encode('ascii','ignore')\n",
    "        \n",
    "        # description\n",
    "        des = soup.find('div', id='description')\n",
    "        descriptions = des.find('pre').text.split('\\n')\n",
    "        descrip = ''\n",
    "        for d in descriptions:\n",
    "            if ':' in d:\n",
    "                try:\n",
    "                    ele = d.split(': ')\n",
    "                    result[ele[0].encode('ascii','ignore')] = ele[1].encode('ascii','ignore')\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                descrip += d\n",
    "        result['description'] = descrip.encode('ascii','ignore')\n",
    "        \n",
    "        # feature\n",
    "        feature = soup.find_all('ul', class_='list-vertical')\n",
    "        for f in feature:\n",
    "            try:\n",
    "                result['property_type'] = f.find('strong').text.encode('ascii','ignore')\n",
    "            except:\n",
    "                result['features'] = f.text.encode('ascii','ignore').strip().split('\\n')\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loop through one page\n",
    "def onepage(url):\n",
    "    source = requests.get(url)\n",
    "    text = source.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "    soup = Soup(text, 'lxml')\n",
    "\n",
    "    cardlike_div = soup.find('ul', class_='search-results__results')\n",
    "    if cardlike_div != None:\n",
    "        cards = cardlike_div.find_all('li', class_='search-results__listing')\n",
    "        for c in cards:\n",
    "            try:\n",
    "                link = c.find('a', class_='listing-result__address')\n",
    "                latitude = c(itemprop=\"latitude\")[0]['content']\n",
    "                longitude = c(itemprop=\"longitude\")[0]['content']\n",
    "                href = link.get('href')\n",
    "                result = scrape_one_page(href)\n",
    "                time.sleep(1)\n",
    "                result['link'] = href\n",
    "                result['latitude'] = latitude\n",
    "                result['longitude'] = longitude\n",
    "                \n",
    "                db.save(result)\n",
    "                print 'Save one!'\n",
    "            except:\n",
    "                print 'ERROR'\n",
    "                continue\n",
    "\n",
    "\n",
    "# main\n",
    "\n",
    "couch = couchdb.Server()\n",
    "couch.resource.credentials = ('admin', '2017')\n",
    "db = couch.create('raw_house')\n",
    "\n",
    "# suburbs\n",
    "suburbs = []\n",
    "with open('data/suburb_region.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        new = ''.join(row[:-1])\n",
    "        split = new.split()\n",
    "        suburbs.append('-'.join(split[:-1] + ['vic'] + split[-1:])) \n",
    "        \n",
    "# attribute setup\n",
    "bedrooms = ['1', '2', '3', '4', '5']\n",
    "bathrooms = ['1', '2', '3']\n",
    "carspace = ['0', '1', '2']\n",
    "\n",
    "# https://www.domain.com.au/sold-listings/?bedrooms=2&bathrooms=1&carspaces=1&suburb=Kensington-vic-3031\n",
    "base_url = 'https://www.domain.com.au/sold-listings/?'\n",
    "\n",
    "\n",
    "# loop through posrcodes\n",
    "for sub in suburbs:\n",
    "    for bed in bedrooms:\n",
    "        for car in carspace:\n",
    "            # find paginator\n",
    "            url = base_url + 'bedrooms=' + bed + '&carspaces=' + car + '&suburb=' + sub\n",
    "            response = requests.get(url)\n",
    "            soup = Soup(response.text, 'lxml')\n",
    "            paginator = []\n",
    "            for div in soup.find_all('a', class_='paginator__page-button'):\n",
    "                paginator.append(int(div.text))\n",
    "\n",
    "            # loop\n",
    "            if paginator != []:\n",
    "                totalpage = max(paginator)\n",
    "                for p in range(totalpage):\n",
    "                    url_sub = url + '&page=' + str(p+1)\n",
    "                    onepage(url_sub)\n",
    "                    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
